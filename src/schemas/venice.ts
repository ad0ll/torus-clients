import z from 'zod';

const userMessageSchema = z.object({
  role: z.literal('user').describe('The role of the message author.'),
  content: z.string().describe('The content of the message.'),
  name: z.string().optional().describe('An optional name for the participant.'),
});

const assistantMessageSchema = z.object({
  role: z.literal('assistant').describe('The role of the message author.'),
  content: z.string().nullable().describe('The content of the message.'),
  name: z.string().optional().describe('An optional name for the participant.'),
  reasoning_content: z.string().nullable().optional().describe('The reasoning content for the message.'),
  tool_calls: z.array(z.any().nullable()).optional().describe('The tool calls generated by the model.'),
});

const toolMessageSchema = z.object({
  role: z.literal('tool').describe('The role of the message author.'),
  content: z.string().describe('The content of the message.'),
  tool_call_id: z.string().describe('The ID of the tool call.'),
  name: z.string().optional().describe('An optional name for the participant.'),
  reasoning_content: z.string().nullable().optional().describe('The reasoning content for the message.'),
  tool_calls: z.array(z.any().nullable()).optional().describe('The tool calls generated by the model.'),
});

const systemMessageSchema = z.object({
  role: z.literal('system').describe('The role of the message author.'),
  content: z.string().describe('The content of the message.'),
  name: z.string().optional().describe('An optional name for the participant.'),
});

const messageSchema = z
  .union([userMessageSchema, assistantMessageSchema, toolMessageSchema, systemMessageSchema])
  .describe('A list of messages comprising the conversation so far.');

const toolSchema = z.object({
  type: z.literal('function').describe("The type of the tool. Currently, only 'function' is supported."),
  function: z
    .object({
      description: z.string().optional().describe('A description of what the function does.'),
      name: z.string().describe('The name of the function to be called.'),
      parameters: z.object({}).describe('The parameters the functions accepts, described as a JSON Schema object.'),
    })
    .describe('The function that the model may call.'),
  id: z.string().optional().describe('An optional ID for the tool.'),
});

export const veniceChatCompletionsRequestSchema = z.object({
  messages: z
    .array(messageSchema)
    .min(1)
    .describe(
      'A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text and images. For compatibility purposes, the schema supports submitting multiple image_url messages, however, only the last image_url message will be passed to and processed by the model.',
    ),
  model: z
    .string()
    .describe(
      'The ID of the model you wish to prompt. May also be a model trait, or a model compatibility mapping. See the models endpoint for a list of models available to you. You can use feature suffixes to enable features from the venice_parameters object. Please see "Model Feature Suffix" documentation for more details. (example: "venice-uncensored")',
    ),
  frequency_penalty: z
    .number()
    .min(-2)
    .max(2)
    .optional()
    .describe(
      "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
    ),
  logprobs: z
    .boolean()
    .optional()
    .describe('Whether to include log probabilities in the response. This is not supported by all models. (example: true)'),
  top_logprobs: z
    .number()
    .min(0)
    .optional()
    .describe('The number of highest probability tokens to return for each token position. (example: 1)'),
  max_completion_tokens: z
    .number()
    .optional()
    .describe(
      'An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.',
    ),
  max_temp: z.number().min(0).max(2).optional().describe('Maximum temperature value for dynamic temperature scaling. (example: 1.5)'),
  max_tokens: z
    .number()
    .optional()
    .describe(
      'The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API. This value is now deprecated in favor of max_completion_tokens.',
    ),
  min_p: z
    .number()
    .min(0)
    .max(1)
    .optional()
    .describe(
      'Sets a minimum probability threshold for token selection. Tokens with probabilities below this value are filtered out. (example: 0.05)',
    ),
  min_temp: z.number().min(0).max(2).optional().describe('Minimum temperature value for dynamic temperature scaling. (example: 0.1)'),
  n: z
    .number()
    .default(1)
    .optional()
    .describe(
      'How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.',
    ),
  presence_penalty: z
    .number()
    .min(-2)
    .max(2)
    .optional()
    .describe(
      "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
    ),
  repetition_penalty: z
    .number()
    .min(0)
    .optional()
    .describe('The parameter for repetition penalty. 1.0 means no penalty. Values > 1.0 discourage repetition. (example: 1.2)'),
  seed: z
    .number()
    .min(1)
    .optional()
    .describe('The random seed used to generate the response. This is useful for reproducibility. (example: 42)'),
  stop: z
    .union([z.string(), z.array(z.string())])
    .nullable()
    .optional()
    .describe('Up to 4 sequences where the API will stop generating further tokens.'),
  stop_token_ids: z
    .array(z.number())
    .optional()
    .describe('Array of token IDs where the API will stop generating further tokens. (example: [151643, 151645])'),
  stream: z.boolean().optional().describe('Whether to stream back partial progress. Defaults to false. (example: true)'),
  stream_options: z
    .object({
      include_usage: z.boolean().optional().describe("If set, an additional chunk will be streamed before the 'data: [DONE]' message."),
    })
    .optional()
    .describe('Options for streaming.'),
  temperature: z
    .number()
    .min(0)
    .max(2)
    .optional()
    .describe(
      'What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both. (example: 0.7)',
    ),
  top_k: z
    .number()
    .min(0)
    .optional()
    .describe('The number of highest probability vocabulary tokens to keep for top-k-filtering. (example: 40)'),
  top_p: z
    .number()
    .min(0)
    .max(1)
    .optional()
    .describe(
      'An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. (example: 0.9)',
    ),
  user: z
    .string()
    .optional()
    .describe('This field is discarded on the request but is supported in the Venice API for compatibility with OpenAI clients.'),
  venice_parameters: z
    .object({
      character_slug: z.string().optional(),
      strip_thinking_response: z.boolean().optional(),
      disable_thinking: z.boolean().optional(),
      enable_web_search: z.enum(['auto', 'off', 'on']).optional(),
      enable_web_citations: z.boolean().optional(),
      include_search_results_in_stream: z.boolean().optional(),
      include_venice_system_prompt: z.boolean().optional(),
    })
    .optional()
    .describe("Unique parameters to Venice's API implementation."),
  parallel_tool_calls: z.boolean().optional().describe('Whether to enable parallel function calling during tool use. (example: false)'),
  response_format: z
    .object({
      type: z.enum(['json_schema', 'json_object']).optional(),
      json_schema: z
        .any()
        // .object({
        //   name: z.string(),
        //   properties: z.record(z.unknown()),
        //   required: z.array(z.string()),
        //   type: z.literal("object"),
        // })
        .optional(),
    })
    .optional()
    .describe('Format in which the response should be returned.'),
  tool_choice: z
    .union([
      z.literal('none'),
      z.literal('auto'),
      z.object({
        type: z.literal('function'),
        function: z.object({
          name: z.string(),
        }),
      }),
    ])
    .optional()
    .describe('Controls which function is called by the model.'),
  tools: z
    .array(toolSchema)
    .nullable()
    .optional()
    .describe(
      'A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for.',
    ),
});

export type VeniceChatCompletionsRequest = z.infer<typeof veniceChatCompletionsRequestSchema>;

const responseAssistantMessageSchema = z.object({
  role: z.literal('assistant'),
  content: z.string().nullable(),
  reasoning_content: z.string().nullable(),
  tool_calls: z.array(z.any()),
});

export const veniceUsageSchema = z.object({
  completion_tokens: z.number().int(),
  prompt_tokens: z.number().int(),
  total_tokens: z.number().int(),
  prompt_tokens_details: z.object({}).nullable(),
});

const choiceSchema = z.object({
  finish_reason: z.string().describe('The reason the model stopped generating tokens.'),
  index: z.number().describe('The index of the choice in the list of choices.'),
  logprobs: z.any().nullable().describe('Log probability information for the choice.'),
  message: z.union([responseAssistantMessageSchema, toolMessageSchema]).describe('A chat completion message generated by the model.'),
  stop_reason: z.any().nullable().describe('The reason the model stopped generating tokens.'),
});

export const veniceChatCompletionsOkResponseSchema = z.object({
  id: z.string().describe("The ID of the request. (example: 'chatcmpl-abc123')"),
  object: z.literal('chat.completion').describe("The type of the object returned. (example: 'chat.completion')"),
  created: z.number().describe('The time at which the request was created. (example: 1677858240)'),
  model: z.string().describe("The model id used for the request. (example: 'venice-uncensored')"),
  choices: z.array(choiceSchema).describe('A list of chat completion choices. Can be more than one if n is greater than 1.'),
  usage: veniceUsageSchema.describe('Usage statistics for the completion request.'),
  prompt_logprobs: z.any().nullable().describe('Log probability information for the prompt.'),
  venice_parameters: z
    .object({
      enable_web_search: z.enum(['auto', 'off', 'on']).describe("Did the request enable web search? (example: 'auto')"),
      enable_web_citations: z.boolean().describe('Did the request enable web citations? (example: true)'),
      include_venice_system_prompt: z.boolean().describe('Did the request include the Venice system prompt? (example: true)'),
      include_search_results_in_stream: z.boolean().describe('Did the request include search results in the stream? (example: false)'),
      strip_thinking_response: z.boolean().describe('Did the request strip thinking response? (example: true)'),
      disable_thinking: z.boolean().describe('Did the request disable thinking? (example: true)'),
      character_slug: z.string().optional().describe("The character slug of a public Venice character. (example: 'venice')"),
      web_search_citations: z
        .array(
          z.object({
            title: z.string().describe('The title of the citation.'),
            url: z.string().describe('The URL of the citation.'),
            content: z.string().optional().describe('The content of the citation.'),
            date: z.string().optional().describe('The date of the citation.'),
          }),
        )
        .optional()
        .describe('Citations from web search results.'),
    })
    .describe("Unique parameters to Venice's API implementation."),
});

export type VeniceChatCompletionsOkResponse = z.infer<typeof veniceChatCompletionsOkResponseSchema>;

export const veniceChatCompletionsBadRequestResponseSchema = z.object({
  error: z.string().describe('A description of the error.'),
  details: z
    .object({ _errors: z.array(z.string()) })
    .passthrough()
    .optional()
    .describe('Details about the incorrect input.'),
});

export type VeniceChatCompletionsBadRequestResponse = z.infer<typeof veniceChatCompletionsBadRequestResponseSchema>;

export const veniceChatCompletionsErrorResponseSchema = z.object({
  error: z.string().describe('A description of the error.'),
});

export type VeniceChatCompletionsErrorResponse = z.infer<typeof veniceChatCompletionsErrorResponseSchema>;

export const veniceChatCompletionsResponseSchema = z.union([
  veniceChatCompletionsOkResponseSchema,
  veniceChatCompletionsBadRequestResponseSchema,
  veniceChatCompletionsErrorResponseSchema,
]);

export type VeniceChatCompletionsResponse =
  | VeniceChatCompletionsOkResponse
  | VeniceChatCompletionsBadRequestResponse
  | VeniceChatCompletionsErrorResponse;

const pricingValueSchema = z.object({
  usd: z.number(),
  vcu: z.number(),
  diem: z.number(),
});

export const veniceModelSchema = z.object({
  created: z.number(),
  id: z.string(),
  model_spec: z.object({
    availableContextTokens: z.number(),
    capabilities: z.object({
      optimizedForCode: z.boolean(),
      quantization: z.string(),
      supportsFunctionCalling: z.boolean(),
      supportsReasoning: z.boolean(),
      supportsResponseSchema: z.boolean(),
      supportsVision: z.boolean(),
      supportsWebSearch: z.boolean(),
      supportsLogProbs: z.boolean(),
    }),
    constraints: z.object({
      temperature: z.object({
        default: z.number(),
      }),
      top_p: z.object({
        default: z.number(),
      }),
    }),
    name: z.string(),
    modelSource: z.string(),
    offline: z.boolean(),
    pricing: z.object({
      input: pricingValueSchema,
      output: pricingValueSchema,
    }),
    traits: z.array(z.string()),
  }),
  object: z.string(),
  owned_by: z.string(),
  type: z.string(),
});

export const veniceModelsResponseSchema = z.object({
  data: z.array(veniceModelSchema),
  object: z.string(),
  type: z.string(),
});

export type VeniceModel = z.infer<typeof veniceModelSchema>;
